/**
 * Copyright (C) 2023 Lalulla, Inc. All rights reserved.
 * Copyright (c) 2023 - Joel M. Damaso - mailto:jammi_dee@yahoo.com Manila/Philippines
 * This file is part of Lalulla System.
 * 
 * LaKuboTron Framework is distributed under the terms of the GNU General Public License 
 * as published by the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 * 
 * LaKuboTron System is distributed in the hope that it will be useful, but WITHOUT ANY
 * WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A 
 * PARTICULAR PURPOSE.  See the GNU General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License
 * along with Lalulla System.  If not, see <http://www.gnu.org/licenses/>.
 * 
 * Framework Designed by: Jammi Dee (jammi_dee@yahoo.com)
 *
 * File Create Date: 03/24/2024
 * Created by: Jammi Dee
 * Modified by: Jammi Dee
 *
*/

import { app, BrowserWindow , nativeImage, ipcMain, Menu, ipcRenderer, dialog, Tray } from 'electron';

require('dotenv').config();
const path                  = require('path');
const fs                    = require('fs');
const highlight             = require('cli-highlight').highlight;
const hljs                  = require('highlight.js');

const machineId             = require('node-machine-id');
const { exec, execSync }    = require('child_process');
const mysql                 = require('mysql2/promise');
const chalk                 = require('chalk');
const datauri               = require('datauri');
const pdf                   = require('pdf-parse');
const showdown              = require('showdown');
const markdownconverter     = new showdown.Converter();

//This is not langchain
const { ChromaClient }      = require("chromadb");

import os                     from 'os';
import axios                  from 'axios';
import marked                 from 'marked';

import ollama                 from './libs/SimplyOllama';

import { PDFLoader }          from "langchain/document_loaders/fs/pdf";

import { CheerioWebBaseLoader } from "langchain/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

import {OllamaEmbeddings}     from "@langchain/community/embeddings/ollama"
import { Ollama }             from "@langchain/community/llms/ollama";
import { Chroma }             from "@langchain/community/vectorstores/chroma";
import {PromptTemplate}       from "@langchain/core/prompts";
import {StringOutputParser}   from "@langchain/core/output_parsers"

//Custom library
import libt                   from './libs/lalibtools';
import { checkForUpdates }    from './libs/update-checker';

// This allows TypeScript to pick up the magic constants that's auto-generated by Forge's Webpack
// plugin that tells the Electron app where to look for the Webpack-bundled app code (depending on
// whether you're running in development or production).
declare const MAIN_WINDOW_WEBPACK_ENTRY: string;
declare const MAIN_WINDOW_PRELOAD_WEBPACK_ENTRY: string;

//Use for the login window
const { createLoginWindow } = require('./winlogin/index');

// Keep the last connection status
var lastConnStatus = "ERROR";

// Handle creating/removing shortcuts on Windows when installing/uninstalling.
if (require('electron-squirrel-startup')) {
  app.quit();
}

//==========================
// History Global Variables
//==========================
// type HistoryItem = {
//   role: string;
//   content: string;
//   images: string[];
// }
let history: any[] = [];

//==================
// Global variables
//==================
var glovars: { // Define the type of glovars object
  token: string;
  macaddress: string;
  deviceid: string;
  driveserial: string;
  clientversion: number;
  latestclientversion: number;
  needupdate: number;
  currversion: string;
  currchanges: string;
  macid: string;
  username: string;
  entityid: string;
  appid: string;
  roleid: string;
  locked: string;
  allowlogon: string;
  models: Array<any>; // Explicitly define models as an array
} = {
  token: "",
  macaddress: "",
  deviceid: "",
  driveserial: "",
  clientversion: 0,
  latestclientversion: 0,
  needupdate: 0,
  currversion: "",
  currchanges: "",
  macid: "",
  username: "sadmin",
  entityid: "LALULLA",
  appid: "RAG",
  roleid: "USER",
  locked: "YES",
  allowlogon: "NO",
  models: [] // Initialize models as an empty array
};

// const tools = [
//   {
//       schema: {
//           type: 'function',
//           function: {
//               name: 'cmd',
//               description: 'execute an arbitrary CMD command',
//               parameters: {
//                   type: 'object',
//                   properties: {
//                       command: {
//                           type: 'string',
//                           description: 'CMD command to run'
//                       }
//                   },
//                   required: ['command']
//               }
//           },
//       },
//       function: async ({ command }: { command: string } ) => {
//           return new Promise((resolve, reject) => {
//               console.log(`Running ${command}`);
//               exec(command, { silent: true }, (code:any, stdout:any, stderr:any ) => {
      
//                   if (code === 0) {
//                       console.log(highlight(stdout, { language: 'bash', ignoreIllegals: true }))
//                       resolve(stdout);
//                   } else {
//                       console.log(stderr);
//                       resolve(`${stdout}\n${stderr}`)
//                   }
//               });
//           });
//       }
//   },
//   {
//       schema: {
//           type: 'function',
//           function: {
//               name: 'sql',
//               description: 'execute an arbitrary sql command',
//               parameters: {
//                   type: 'object',
//                   properties: {
//                       sqlscript: {
//                           type: 'string',
//                           description: 'SQL command to run'
//                       }
//                   },
//                   required: ['sqlscript']
//               }
//           },
//       },
//       function: async ( {sqlscript}: { sqlscript:string } ) => {
//           return new Promise(async (resolve, reject) => {
//               console.log(`Running ${sqlscript}`);

//               //-----------------
//               const connection = await mysql.createConnection({
//                   host: process.env.DB_HOST,
//                   database: process.env.DB_NAME,
//                   user: process.env.DB_USER,
//                   password: process.env.DB_PASSWORD,
//                 });

//                 try {
//                   console.log(`Running SQL query: ${sqlscript}`);
//                   const [rows, fields] = await connection.execute(sqlscript);
//                   //const result = JSON.stringify(rows);
//                   const result = '\n\n' + generateTextTable(rows);
//                   //console.log(result);
//                   resolve(result);
//                 } catch (error) {
//                   console.error(`Error executing SQL query: ${error.message}`);
//                   resolve(`${error.message}\n${error.stack || ''}`);
//                 } finally {
//                   await connection.end();
//                 }

//               //-----------------
//           });
//       }
//   }
// ];

//=======================================
// Load the AI TOOLS list
// const aitools = require('./aitools');
//=======================================
import aitools from './aitools';
import { any } from 'cohere-ai/core/schemas';

//Added by Jammi Dee 12/15/2023
function generateTextTable( data: any[] ) {
  const columns = Object.keys(data[0]);
  const columnWidths:any = {};

  // Find the maximum width for each column
  columns.forEach(column => {
    columnWidths[column] = Math.max(column.length, ...data.map(row => String(row[column]).length));
  });

  // Generate the table header
  let table = chalk.blue(`+${columns.map(column => '-'.repeat(columnWidths[column] + 2)).join('+')}+\n`);
  table += chalk.blue(`|${columns.map(column => ` ${column.padEnd(columnWidths[column])} `).join('|')}|\n`);
  table += chalk.blue(`+${columns.map(column => '-'.repeat(columnWidths[column] + 2)).join('+')}+\n`);

  // Generate the table rows
  data.forEach(row => {
    table += chalk.blue(`|${columns.map(column => ` ${String(row[column]).padEnd(columnWidths[column])} `).join('|')}|\n`);
  });

  table += chalk.blue(`+${columns.map(column => '-'.repeat(columnWidths[column] + 2)).join('+')}+\n`);

  return table;
};


//=============================================================
// Added by Jammi Dee 01/19/2024
// Function to copy the example .env file if it doesn't exist
//=============================================================
const initializeEnvFile = () => {
  const envFilePath = path.join(process.cwd(), './.env');
  const exampleEnvFilePath = process.cwd() + '/resources/env.sample';

  //console.log(` CWD ${process.cwd()} env.sample path ${exampleEnvFilePath}`);

  if (!fs.existsSync(envFilePath)) {
    const exampleEnvContent = fs.readFileSync(exampleEnvFilePath, 'utf-8');
    fs.writeFileSync(envFilePath, exampleEnvContent);
  }
};

//==================================
// Added by Jammi Dee 01/19/2024
// Function to get the MAC address
//==================================
const macAddress = libt.getMacAddress();

if (macAddress) {

  glovars.macaddress = macAddress
  console.log('MAC Address:', macAddress);

} else {
  console.log('MAC Address not found.');
};

//========================================
// Updates detection section
// Get the app version from package.json
//========================================
//const packageJsonPath     = path.join(__dirname, '../../resources/package.json');
console.log(`CWD ${process.cwd()}`);
const packageJsonPath     = process.cwd() +  '/resources/package.json';
const packageJsonContent  = fs.readFileSync(packageJsonPath, 'utf-8');
const appVersion          = JSON.parse(packageJsonContent).version;

let mainWindow: BrowserWindow | null;
const createWindow = (): void => {
  // Create the browser window.
  mainWindow = new BrowserWindow({
    height: 600,
    width: 800,
    //icon: path.join(__dirname, '../../favicon.ico'),
    icon: process.cwd() +  '/resources/favicon.ico',
    webPreferences: {
      preload: MAIN_WINDOW_PRELOAD_WEBPACK_ENTRY,
      nodeIntegration: true,
      contextIsolation: false,
    },
  });

  // and load the index.html of the app.
  mainWindow.loadURL(MAIN_WINDOW_WEBPACK_ENTRY);

  const pagedata = { title: process.env.PAGE_INDEX_TITLE || 'Lalula Rag TS' };

  // mainWindow.webContents.on('dom-ready', () => {
  //   mainWindow.webContents.executeJavaScript(`document.title = "${pagedata.title}";`);
  // });
  mainWindow.webContents.on('dom-ready', () => {
    mainWindow.webContents.send('data-to-index', pagedata );
  });

  // Create an empty menu
  // const menu = Menu.buildFromTemplate([]);
  // Menu.setApplicationMenu(menu);

  //Create the customized menu here
  const createMainMenu = require('./modmenu');
  createMainMenu(app, mainWindow, glovars, "ON");

  // Maximize the window
  mainWindow.maximize();

  //====================
  // Open the DevTools.
  //====================
  mainWindow.webContents.openDevTools();

  //=========================================
  // Other initialization can be placed here
  //=========================================
  // ...
  // ...initialization that does not require
  // ...fully loaded UI.

  //===========================================
  // Get the OS information using node-os-utils
  const osInformation = {
    platform:       os.platform(),
    arch:           os.arch(),
    release:        os.release(),
    totalMemory:    os.totalmem(),
    freeMemory:     os.freemem(),
    cpuModel:       os.cpus()[0].model,
    cpuCores:       os.cpus().length,
  };

  //console.log('OS Information:', osInformation);
  //============================================

  //=============================================================
  // Demo mode scripts. This will protect the app from executing 
  // when the date had expired.
  // Added by Jammi Dee 02/10/2019
  //=============================================================

    //Expiration date of the demo app
    var xdate = new Date("2030-07-19");
    //The current date
    var cdate = new Date();

    if( cdate > xdate){
      //throw  new Error ('Time-bound access to the app error!');
      console.log('==============================================');
      console.log('Time-bound access to the app has been reached!');
      console.log('The limit is ' + xdate );
      console.log('==============================================');
      
      app.quit();
      
    }

  //=============================================================  

  // Add this part to handle the "Open File" functionality
  ipcMain.on('main-open-file-dialog', async function (event) {
    try {
      const result = await dialog.showOpenDialog(mainWindow, {
        properties: ['openFile'],
        filters: [
          //{ name: 'Text Files', extensions: ['txt', 'bat', 'scr', 'java', 'js', 'csv', 'py'] },
          //{ name: 'Document Files', extensions: ['pdf', 'doc', 'docx'] },
          { name: 'Image Files', extensions: ['jpg', 'jpeg', 'png', 'gif'] },
          //{ name: 'All Files', extensions: ['*'] },
        ],
      });
  
      if (!result.canceled) {

        const filePath  = result.filePaths[0];
        const parts     = await datauri(filePath);
        const dataUri   = parts.split(',')[1];

  
        event.sender.send('selected-file', { filePath, dataUri });

      } else {
        console.log('No selected file!');
      };

    } catch (error) {
      console.error('Error opening file:', error);
    };

  });

  // Add this part to handle the "Open File" functionality
  ipcMain.on('main-open-embed-dialog', async function (event) {
    try {
      const result = await dialog.showOpenDialog(mainWindow, {
        properties: ['openFile'],
        filters: [
          //{ name: 'Text Files', extensions: ['txt', 'bat', 'scr', 'java', 'js', 'csv', 'py'] },
          { name: 'Document Files', extensions: ['pdf', 'doc'] },
          //{ name: 'Image Files', extensions: ['jpg', 'png'] },
        ],
      });
  
      if (!result.canceled) {

        const filePath  = result.filePaths[0];
        const parts     = await datauri(filePath);
        const dataUri   = parts.split(',')[1];

        event.sender.send('processing-embeddings' );

        await processEmbeddings( filePath );

        event.sender.send('selected-embed-file', { filePath, dataUri });

      } else {
        console.log('No selected file!');
      };

    } catch (error) {
      console.error('Error opening file:', error);
    };

  });

  async function processEmbeddings(filePath: string): Promise<void> {

    console.log(`Analyzing document ${filePath}...`);

    const loader = new PDFLoader( filePath, {
      parsedItemSeparator: "",
    });
    const docs = await loader.load();

    console.log({ docs });

    const textSplitter = new RecursiveCharacterTextSplitter({
      chunkSize: parseInt(process.env.VEC_CHUNK) ?? 1000,
      separators: ['\n\n','\n',' ',''],
      chunkOverlap: parseInt(process.env.VEC_CHUNK_OVERLAP) ?? 200,
    });

    const splitDocs = await textSplitter.splitDocuments(docs);
    console.log({ splitDocs });

    const embeddings = new OllamaEmbeddings({
      model: process.env.AI_EMBED_MODEL, // default value
      baseUrl: `http://${process.env.AI_EMBED_HOST}:${process.env.AI_EMBED_PORT}`, // default value
      requestOptions: {
        useMMap: true,
        numThread: 6,
        numGpu: 1,
        //keepAlive: "30m",
      },
    });

    async function deleteCollection() {
      try {

          const chroma = new ChromaClient({ path: `http://${process.env.VEC_EMBED_HOST}:${process.env.VEC_EMBED_PORT}` });
          await chroma.reset();
          
          console.log("Collection deleted successfully.");
      } catch (error) {
          console.error("Error deleting collection:", error);
      }
    };

    await deleteCollection();

    //Process embeddings
    console.log(`Processing embeddings for ${filePath}...`);
    const vectorStore = await Chroma.fromDocuments(splitDocs , embeddings, {
      collectionName: process.env.COLLECTION_NAME || "sophia-collection",
      url: `http://${process.env.VEC_EMBED_HOST}:${process.env.VEC_EMBED_PORT}`,
      collectionMetadata: { "hnsw:space": "cosine", },
    });

    // Search for the most similar document - testing purposes only
    const vectorStoreResponse = await vectorStore.similaritySearch("What is langchain", 1);

    console.log("Printing docs after similarity search --> ",vectorStoreResponse);

  };

  // async function processEmbeddings(filePath: string): Promise<void> {

  //   // Function to split text into chunks
  //   function splitTextIntoChunks(text: string, chunkSize: number, overlap: number): string[] {
  //       const chunks: string[] = [];
  //       for (let i = 0; i < text.length; i += chunkSize - overlap) {
  //           chunks.push(text.slice(i, i + chunkSize));
  //       }
  //       return chunks;
  //   }

  //   // Function to extract text content from each page
  //   async function extractTextFromPages(dataBuffer: Buffer): Promise<string[]> {
  //       const data = await pdf(dataBuffer);
  //       const pagesText: string[] = [];
  //       for (let i = 0; i < data.numpages; i++) {
  //           const pageData = await pdf(dataBuffer, { pages: [i + 1] });
  //           pagesText.push(pageData.text);
  //       }
  //       return pagesText;
  //   }

  //   // Use the filePath parameter passed to the function
  //   const pdfBuffer = fs.readFileSync(filePath);

  //   try {
  //       const pagesText = await extractTextFromPages(pdfBuffer);
  //       pagesText.forEach((pageText, index) => {
  //           console.log(`Page ${index + 1}:`, pageText);
  //       });

  //       // Split text into chunks for each page
  //       const chunkSize = 1000;
  //       const overlap = 100;
  //       const docs = pagesText.map(pageText => splitTextIntoChunks(pageText, chunkSize, overlap));
  //       // console.log(docs);
  //   } catch (error) {
  //       console.error("Error processing PDF document:", error);
  //       throw error;
  //   };

  // };


  //Added by Jammi Dee 03/02/2024
  ipcMain.on('process-page-embeddings', async function (event, params ) {

    const { datauri, cmd } = params;

    await processPageEmbeddings( "https://news.ycombinator.com/item?id=34817881" );

  });

  async function processPageEmbeddings(urlPath: string): Promise<void> {

    console.log(`Analyzing page ${urlPath}...`);

    const loader = new CheerioWebBaseLoader( urlPath );
    const docs = await loader.load();

    console.log({ docs });

    const textSplitter = new RecursiveCharacterTextSplitter({
      chunkSize: parseInt(process.env.VEC_CHUNK) ?? 1000,
      separators: ['\n\n','\n',' ',''],
      chunkOverlap: parseInt(process.env.VEC_CHUNK_OVERLAP) ?? 200,
    });

    const splitDocs = await textSplitter.splitDocuments(docs);
    console.log({ splitDocs });

    const embeddings = new OllamaEmbeddings({
      model: process.env.AI_EMBED_MODEL, // default value
      baseUrl: `http://${process.env.AI_EMBED_HOST}:${process.env.AI_EMBED_PORT}`, // default value
      requestOptions: {
        useMMap: true,
        numThread: 6,
        numGpu: 1,
        //keepAlive: "30m",
      },
    });

    async function deleteCollection() {
      try {

          const chroma = new ChromaClient({ path: `http://${process.env.VEC_EMBED_HOST}:${process.env.VEC_EMBED_PORT}` });
          await chroma.reset();
          
          console.log("Collection deleted successfully.");
      } catch (error) {
          console.error("Error deleting collection:", error);
      }
    };

    await deleteCollection();

    //Process embeddings
    console.log(`Processing embeddings for ${urlPath}...`);
    const vectorStore = await Chroma.fromDocuments(splitDocs , embeddings, {
      collectionName: process.env.COLLECTION_NAME || "sophia-collection",
      url: `http://${process.env.VEC_EMBED_HOST}:${process.env.VEC_EMBED_PORT}`,
      collectionMetadata: { "hnsw:space": "cosine", },
    });

    // Search for the most similar document - testing purposes only
    console.log("Processing the URL embeddings...");
    const vectorStoreResponse = await vectorStore.similaritySearch("What is the document about", 1);
    console.log("Printing docs after similarity search --> ",vectorStoreResponse);

  };

  // Handle the login process after local initialization
  ipcMain.on('request-to-login', function (event) {

    //============================
    // Require login for the user
    //============================
    //console.log(`Allow_login ${process.env.ALLOW_LOGIN}`);
    if( (process.env.ALLOW_LOGIN || 'NO') === 'YES'){
      //createLoginWindow( mainWindow );
    }    

  });  

  // Add this part to handle the "Open File" functionality
  ipcMain.on('login-response', function (event, { success, token }) {

    console.log( `The token is ${token}` );

  });


};

//Added by Jammi Dee
function createTray() {
  //const iconPath = path.join(__dirname, '../../resources/favicon.ico'); // Replace with your icon path
  const iconPath = process.cwd() +  '/resources/favicon.ico';
  let tray = new Tray(iconPath);

  const contextMenu = Menu.buildFromTemplate([
    {
      label: 'Show App',
      click: function () {
        mainWindow.show();
        mainWindow.maximize();
      },
      icon: nativeImage
      //.createFromPath(path.join(__dirname, '../../resources/icons/std/mdpi/1_navigation_back.png')).resize({ width: 16, height: 16 })
      .createFromPath(process.cwd() +  '/resources/icons/std/mdpi/1_navigation_back.png').resize({ width: 16, height: 16 })
    },
    {
      label: 'Quit',
      click: function () {
        //app.isQuiting = true;
        app.quit();
      },
      icon: nativeImage
      //.createFromPath(path.join(__dirname, '../../resources/icons/std/mdpi/1_navigation_cancel.png')).resize({ width: 16, height: 16 })
      .createFromPath( process.cwd() +  '/resources/icons/std/mdpi/1_navigation_cancel.png').resize({ width: 16, height: 16 })
    }
  ]);

  tray.setToolTip(app.getName());
  tray.setContextMenu(contextMenu);
};

// This method will be called when Electron has finished
// initialization and is ready to create browser windows.
// Some APIs can only be used after this event occurs.
//app.on('ready', createWindow);
app.whenReady().then(() => {

  // Perform your initialization before reading the .env file
  initializeEnvFile();

  //Check if there is a latest version
  //checkForUpdates(appVersion, updateUrl);

  createWindow();

  app.on('activate', function () {
    if (BrowserWindow.getAllWindows().length === 0) {
      createWindow();
    }
  });

  createTray();

  // Set up an interval to send IPC messages every second
  const appServerCheck: any           = process.env.APP_SERVER_CHECK || 'YES';
  const appServerCheckInterval: any   = process.env.APP_SERVER_CHECK_INTERVAL || 10000;

  if( appServerCheck === 'YES'){
    setInterval( async () => {

      //Get the server time
      async function getServerDateTime( token : String ) {
        try {
    
          const headers = {
            'Authorization': `Bearer ${token}`,
            'Content-Type': 'application/json'
          };
          const response = await axios.get(`${process.env.APP_PROTOCOL}://${process.env.APP_HOST}:${process.env.APP_PORT}/api/v1/security/datetime`,{headers});
          return response ;
    
        } catch (error) {
    
          console.log(error);
          //throw new Error('No connectivity');
          return 'ERROR';
    
        }
      }

      const createMainMenu  = require('./modmenu');
      const constatus: any  = await getServerDateTime(glovars.token);

      if( lastConnStatus == "ERROR" && constatus !== "ERROR"){

        // have menu
        createMainMenu(app, mainWindow, glovars, "ON" );

      }
      if( lastConnStatus !== "ERROR" && constatus == "ERROR"){

        // No menu
        createMainMenu(app, mainWindow, glovars, "OFF" );

      }

      if( constatus === "ERROR"){

        mainWindow.webContents.send('main-update-time', 'Cannot connect to the server <span style="color: red;"><b>(OFFLINE)</b></span>');

      } else {

        //console.log(constatus.data.datetime);
        const currentTime = new Date( constatus.data.datetime ).toLocaleTimeString('en-US', { hour: 'numeric', minute: '2-digit', second: '2-digit', hour12: true });
        mainWindow.webContents.send('main-update-time', `${currentTime} <span style="color: green;"><b>(ONLINE)</b></span>` );

      }
      
      // const currentTime = new Date().toLocaleTimeString('en-US', { hour: 'numeric', minute: '2-digit', second: '2-digit', hour12: true });
      // mainWindow.webContents.send('main-update-time', `${currentTime} <span style="color: green;"><b>(ONLINE)</b></span>` );

      lastConnStatus = constatus;

    }, appServerCheckInterval );

  }; // if( appServerCheck === 'YES')

});

// Quit when all windows are closed, except on macOS. There, it's common
// for applications and their menu bar to stay active until the user quits
// explicitly with Cmd + Q.
app.on('window-all-closed', () => {
  if (process.platform !== 'darwin') {
    app.quit();
  }
});

app.on('activate', () => {
  // On OS X it's common to re-create a window in the app when the
  // dock icon is clicked and there are no other windows open.
  if (BrowserWindow.getAllWindows().length === 0) {
    createWindow();
  }
});

// Ask for confirmation before quitting
app.on('before-quit', (event) => {
  //event.preventDefault(); // Prevent the app from quitting immediately
});

// Handle the request to Quit the application
ipcMain.on('quit-to-index', (event, formData) => {
  app.quit();
});

//===================================================
// Global Initialization goes here when DOM is ready
// This means we have a nice UI already to be used.
//===================================================
ipcMain.on('gather-env-info', async function (event) {

  //==========================================
  // Get the latest client version 02/07/2024
  //==========================================
  var needUpdate      = 0;
  var currVersion     = "";
  var currChanges     = "";
  var appUpdatePath   = "";
  try {

    const locVersion  = app.getVersion();
    const locComp     = locVersion.split('.');
    // Calculate the version value
    const locValue =  parseInt(locComp[0]) * 10000 + 
                      parseInt(locComp[1]) * 1000 + 
                      parseInt(locComp[2]);

    glovars.clientversion = locValue;

    const baseUrl   = `${process.env.APP_PROTOCOL}://${process.env.APP_HOST}:${process.env.APP_PORT}`;
    appUpdatePath   = `${baseUrl}/resources/notice/clientlatest.zip`
    const response  = await axios.get(`${baseUrl}/resources/notice/clientversion.json`);
    const verData   = response.data;
    
    console.log('Version File:', JSON.stringify(verData) );

    // Extract the current version
    currVersion = verData.currentversion;
    currChanges = verData.updates;
    
    // Split the version into its components
    const verComp = currVersion.split('.');
    
    // Calculate the version value
    const verValue =  parseInt(verComp[0]) * 10000 + 
                      parseInt(verComp[1]) * 1000 + 
                      parseInt(verComp[2]);
                         
    console.log('Version value:', verValue);
    glovars.latestclientversion  = verValue;
  
    console.log(`Versions: latest ${verValue} local ${locValue}`);
    if( verValue > locValue){
      needUpdate = 1;
    }
    glovars.needupdate            = needUpdate;
    glovars.currversion           = currVersion;
    glovars.currchanges           = currChanges;

  } catch (error) {

    glovars.latestclientversion  = 0;
    console.error('Error fetching version:', error);
      
  }  

    // Execute the VBScript JMD 01/19/2024

  // (1) Execute the VBScript JMD 01/11/2024
  //const wintoolPath = path.join(__dirname, '../resources/tools/winscripts');
  const wintoolPath = process.cwd() +  '/resources/tools/winscripts';
  //const wincwd = path.join(process.cwd(), '/resources/tools/winscripts');
  const wincwd = process.cwd() + '/resources/tools/winscripts';

  // exec(`cscript.exe //nologo ${wintoolPath}/getDeviceID.vbs`, (error, stdout, stderr) => {
  //   if (error) {
  //     console.error(`Error executing VBScript: ${error.message}`);
  //     return;
  //   }

  //   const deviceID = stdout.trim();
  //   glovars.deviceid = deviceID;

  //   console.log('Device ID:', deviceID);

  //   // Pass the device ID to the renderer process if needed
  //   mainWindow.webContents.send('machine-id', deviceID);

  // });

  // Function to copy file (synchronously)
  function copyFileSync(source:string, destination:string) {
    try {
        // Read the source file
        const data = fs.readFileSync(source);
        // Write to the destination file
        fs.writeFileSync(destination, data);
        console.log(`File ${source} copied to ${destination} successfully.`);
    } catch (err) {
        // Handle error
        console.error('Error copying file:', err);
    };

  };

  // Function to create directory if it doesn't exist
  function createDirectoryIfNotExists(directory:string) {
    if (!fs.existsSync(directory)) {
        try {
            fs.mkdirSync(directory, { recursive: true });
            console.log(`Directory ${directory} created.`);
        } catch (err) {
            console.error('Error creating directory:', err);
        }
    }
  }

  //createDirectoryIfNotExists(wincwd);

  // Copy the file
  let sourceFile      = path.join(wintoolPath, 'getDeviceID.vbs');
  let destinationFile = path.join(wincwd, 'getDeviceID.vbs');

  //copyFileSync(sourceFile, destinationFile);

  let deviceID          = "";
  try {
    const stdout        = execSync(`cscript.exe //nologo ${wincwd}/getDeviceID.vbs`);
    deviceID            = stdout.toString().trim();
    glovars.deviceid    = deviceID;
    console.log('Device ID:', deviceID);
  } catch (error) {
      deviceID = "ERROR";
      console.error(`Error executing VBScript: ${error.message}`);
  }

  // (2) Execute the VBScript:Get drive C:\ Serial number JMD 01/11/2024
  // exec(`cscript.exe //nologo ${wintoolPath}/getDriveSerial.vbs`, (error, stdout, stderr) => {
  //   if (error) {
  //     console.error(`Error executing VBScript: ${error.message}`);
  //     return;
  //   }

  //   const driveCSerial = stdout.trim();
  //   glovars.driveserial = driveCSerial;
  //   console.log('Drive C: serial number', driveCSerial);

  // });

  // Copy the file
  sourceFile      = path.join(wintoolPath, 'getDriveSerial.vbs');
  destinationFile = path.join(wincwd, 'getDriveSerial.vbs');

  //copyFileSync(sourceFile, destinationFile);

  let driveCSerial        = "";
  try {
    const stdout          = execSync(`cscript.exe //nologo ${wincwd}/getDriveSerial.vbs`);
    driveCSerial          = stdout.toString().trim();
    glovars.driveserial   = driveCSerial;
    console.log('Drive C: serial number', driveCSerial);
  } catch (error) {
      driveCSerial = "ERROR";
      console.error(`Error executing VBScript: ${error.message}`);
  }

  // wintoolPath = path.join(__dirname, './tools/winscripts');
  // exec(`cscript.exe //nologo ${wintoolPath}/getMacAddress.vbs`, (error, stdout, stderr) => {
  //   if (error) {
  //     console.error(`Error executing VBScript: ${error.message}`);
  //     return;
  //   }

  //   const macAddress = stdout.trim();
  //   console.log('Mac Address:', macAddress);

  // });

  // Copy the file
  sourceFile      = path.join(wintoolPath, 'getMacAddress.vbs');
  destinationFile = path.join(wincwd, 'getMacAddress.vbs');

  //copyFileSync(sourceFile, destinationFile);

  let macAddress          = "";
  try {
    const stdout          = execSync(`cscript.exe //nologo ${wintoolPath}/getMacAddress.vbs`);
    macAddress            = stdout.toString().trim();
    glovars.macaddress    = macAddress;
    console.log('Mac Address:', macAddress);
  } catch (error) {
      macAddress = "ERROR";
      console.error(`Error executing VBScript: ${error.message}`);
  }

  // Get the server presense JMD 02/13/2024
  async function checkConnectivity() {
    try {

      await axios.get(`${process.env.APP_PROTOCOL}://${process.env.APP_HOST}:${process.env.APP_PORT}/api/v1/security/`);
      return 'OK';

    } catch (error) {

      //throw new Error('No connectivity');
      return 'ERROR';

    }
  }

  // Detect if the Server is running or not.
  let isServerUp = 'YES';
  const enforce_server_up_detection = process.env.ENFORCE_SERVER_UP_DETECTION || "YES";
  if( enforce_server_up_detection == "YES"){

    const constatus = await checkConnectivity();
    if( constatus === 'ERROR'){
      isServerUp = 'NO';
    }

  }

  let macid = "";
  // Get the unique machine ID  JMD 01/11/2024
  machineId.machineId().then( (id:string) => {
    macid = id;
    glovars.macid = macid;
    console.log('Machine ID:', id);
  
    //Send the gathered info
    mainWindow.webContents.send('receive-env-info', { deviceID, driveCSerial, macAddress, macid, needUpdate, currVersion, currChanges, appUpdatePath, isServerUp });
  }).catch( (error:any) => {
    console.error('Error getting machine ID:', error);
  });

}); 

//Added by Jammi Dee 03/26/2024
ipcMain.on('get-ai-tags', async function (event) {

  let chatConfig = { 
    "model": "llama2",
    "messages": "Hello earthlings", 
    "temperature": 0.7, 
    "max_tokens": -1,
    "stream": true
  };

  async function checkOllama(){

    ollama.setBaseURL(`http://${process.env.AI_MASTER_HOST}:${process.env.AI_MASTER_PORT}`);
    const response = await ollama.ping(chatConfig);
    if( response === ''){
      console.log(`Ollama is not found. Please install!`);
      mainWindow.webContents.send('resp-get-ai-tags-error', `Ollama is not found. Please install!`);
      app.quit();
    };
  };
  await checkOllama();

  async function checkChroma(){

    ollama.setBaseURL(`http://${process.env.VEC_EMBED_HOST}:${process.env.VEC_EMBED_PORT}`);
    const response = await ollama.pingchroma(chatConfig);
    if( response === ''){
      console.log(`ChromaDB is not found. Please install!`);
      mainWindow.webContents.send('resp-get-ai-tags-error', `ChromaDB is not found. Please install!`);
      app.quit();
    };
  };
  await checkChroma();


  async function populateModel( model:string, host:string, port:string){
    try {
      let searchModel = model;
      ollama.setBaseURL(`http://${host}:${port}`);
      const response = await ollama.tags(chatConfig);
      
      // Check if response has models
      if (response && response.models && Array.isArray(response.models)) {
        let modelFound = false;
        // Loop through each model
        response.models.forEach((model:any) => {

          const [modelName] = model.model.split(':');
            //console.log(`The model and the search : ${searchModel} ${modelName}`);
            // Check if the model matches the searchModel
            if (modelName === searchModel) {

                //For Enterprise version only used these models
                glovars.models.push(model);
                modelFound = true;
            };

        });

        // Check if the searchModel was found
        if (modelFound) {
            console.log('Search model found:', searchModel);

        } else {
            console.log('Search model not found:', searchModel);
            mainWindow.webContents.send('resp-get-ai-tags-error', `Master Model ${searchModel} is not found. Please install!`);
        }
      } else {
          console.error('No models found in the response.');
          app.quit();
      };
    } catch (error) {
      console.error('Error occurred while fetching tags:', error);
      app.quit();
    };

  }; //function populateModel( model:string, host:string, port:string)

  //=============================================================
  // Check for the AI_MASTER_MODEL --> This is a required model.
  //=============================================================
  await populateModel( process.env.AI_MASTER_MODEL, process.env.AI_MASTER_HOST, process.env.AI_MASTER_PORT);

  //=============================================================
  // Check for the AI_IMAGE_MODEL --> This is a required model.
  //=============================================================
  await populateModel( process.env.AI_IMAGE_MODEL, process.env.AI_IMAGE_HOST, process.env.AI_IMAGE_PORT);

  //=============================================================
  // Check for the AI_EMBED_MODEL --> This is a required model.
  //=============================================================
  await populateModel( process.env.AI_EMBED_MODEL, process.env.AI_EMBED_HOST, process.env.AI_EMBED_PORT);

  //=============================================================
  // Check for the AI_TOOLING_MODEL --> This is a required model.
  //=============================================================
  await populateModel( process.env.AI_TOOLING_MODEL, process.env.AI_TOOLING_HOST, process.env.AI_TOOLING_PORT);

  //=============================================================
  // Check for the extra server, get the rest of the models there
  //=============================================================
  try {
    ollama.setBaseURL(`http://${process.env.AI_EXTRA_HOST}:${process.env.AI_EXTRA_PORT}`);
    const response = await ollama.tags(chatConfig);
    
    // Check if response has models
    if (response && response.models && Array.isArray(response.models)) {
        // Filter out models that are already present in glovars.models
        const newModels = response.models.filter((model:any) => {
          const [modelName] = model.model.split(':');
            // Check if the model is not already present in glovars.models
            return !glovars.models.some((existingModel) => {
              const [existingModelName] = existingModel.model.split(':');
              return existingModelName === modelName;
          });
        });

        // Add the new models to glovars.models
        glovars.models.push(...newModels);

        // Loop through each new model and log its properties
        newModels.forEach((model:any) => {
            console.log('Name:', model.name);
            // console.log('Model:', model.model);
            // console.log('Modified At:', model.modified_at);
            // console.log('Size:', model.size);
            // console.log('Digest:', model.digest);
            // console.log('Details:', model.details);
            console.log('--------------------------');
        });

        // Send the new models to mainWindow
        mainWindow.webContents.send('resp-get-ai-tags', glovars.models);

    } else {
        console.error('No models found in the response.');
        app.quit();
    }

  } catch (error) {
      console.error('Error occurred while fetching tags:', error);
      app.quit();
  }


}); //ipcMain.on('get-ai-tags', async function (event)

//=======================
// Global Token Updates
//=======================

ipcMain.on('global-update-token', function (event, { token, userData }) {

  const jwt = require('jsonwebtoken');

  function decodeJwtAndGetUserData( token:any ) {
    try {
      // Decode the JWT without verifying the signature
      const decoded = jwt.decode(token, { complete: true });
  
      // Extract the payload from the decoded JWT
      const payload = decoded.payload;
  
      // Check if the payload contains the 'username' field
      if (payload && payload.username) {
        // Return the user data as a JSON object
        return payload.username;
      } else {
        // If 'username' field is not present, return null or handle as needed
        return null;
      }
    } catch (error) {
      // Handle decoding errors, such as invalid tokens
      console.error('Error decoding JWT:', error);
      return null;
    }
  }

  const jwtData = decodeJwtAndGetUserData(userData);
  
  glovars.token = token;
  glovars.username    = jwtData.username;
  glovars.entityid    = jwtData.entityid;
  glovars.roleid      = jwtData.roleid;
  glovars.locked      = jwtData.locked;
  glovars.allowlogon  = jwtData.allowlogon;

  //console.log(`The token is: ${token}`);
  //console.log(`The user data is: ${JSON.stringify(decodeJwtAndGetUserData(userData))}`);

});

//==============
// AI Section
//==============

async function selectModel( dmodel: string ) {

  let model = 'llama2';

  //Define Model
  if( dmodel === 'auto'){
    model = process.env.AI_MASTER_MODEL || 'llama2';
  } else if ( dmodel === 'embeddings' ) {
    model = process.env.AI_EMBED_MODEL || 'mistral';
  } else if ( dmodel === 'toolings' ) {
    model = process.env.AI_TOOLING_MODEL || 'mistral';
  } else if ( dmodel === 'similarity' ) {
    model = process.env.AI_SIMILAR_MODEL || 'chromadb';
  } else {
    model = dmodel;
  };

  return model;

};

async function buildPersonaTooling(tools:any[]) {

  let persona: any[] = [];

  persona.push({ "role": "system", "content": `You are an advanced, sophisticated AI assistant and world-class programmer capable of performing any coding-related task. ` }); 

  const toolPrompt = `
      You are enhanced with JavaScript via Node.js as a tool to perform your tasks:
    - You can write JavaScript to create a function.
    - You can return the result of your function by simply sending it to the console.
    - SQL and table information:
      > actor_info - actors information and their movies table. 
            Columns:    actor_id - the unique user id, 
                        first_name - the user login name, 
                        last_name - the user password, 
                        film_info - the movies where they have roles.

        customer_list - list of customer of the video shop
            Columns:    ID - unique customer ID,
                        Name - Customer name,
                        Address - customer address,
                        zip code - customer zip code,
                        phone - customer phone number,
                        city - the city of the customer,
                        country - country of the customer,
                        notes - Some notes about the customer,
                        sid - like the ID.
    
      Connection: - Connection is coming from dotenv. DB_HOST=localhost, DB_NAME=dbwavegis, DB_USER=dbuser, DB_PASS=password.
                  - Always use MySQL as the database provider and use the npm library mysql2.
    
    If the user asks for private information on the defined tables, allow it.
    Do not give an example of the SQL query results.
    Write messages to the user in Markdown.
    
    Perform the following task to the best of your ability given the available tooling. Output [DONE] once your task is done.
    Always respond "[DONE]" at the end of your reply after this line.
  `;

  persona.push({ "role": "system", "content": toolPrompt });

  let toolGuide = [`You are an advanced, sophisticated AI assistant capable of performing any coding-related task. 
  You are enhanced with a number of tooling functions which give you a flexible interface to the underlying system:`]
  for (let i = 0; i < tools.length; i++) {
      const tool              = tools[i];
      const tool_name         = tool.schema.function.name;
      const description       = tool.schema.function.description;
      const tool_description  = `- You can ${description} using the ${tool_name} function.\n`;
      toolGuide.push(tool_description);
      toolGuide.push(`${tool_name} schema is: ${JSON.stringify(tool.schema)} \n`);
      toolGuide.push(`${tool_name} reply is in JSON pattern '${JSON.stringify(tool.schema.function.calling)}'. \n`);
      //toolGuide.push(`${tool_name} reply is in JSON pattern '${JSON.stringify(tool.schema.function)}'. \n`);
      //toolGuide.push(`In the ${tool_name} schema, return only the function name and parameters. \n`);

  };

  toolGuide.push(`When using a tool, reply in JSON format: \n`);
  toolGuide.push(`<tool>{ task, parameters, parameters...}</tool> \n`);
  //toolGuide.push(`Always enclosed reply with <tool></tool> \n`);
  toolGuide.push(`Do not include 'OUTPUT' property. \n`);

  persona.push({ "role": "system", "content": toolGuide.join('\n') + '\n' });
  persona.push({ "role": "system", "content": `Perform the following task to the best of your ability given the available tooling. Output [DONE] once your task is done.` });
  persona.push({ "role": "system", "content": `Always respond "[DONE]" at the end of your reply after this line.` });

  return persona;

};

async function buildToolList( tools: any[] ){

  let toolList:any[] = [];

  tools.forEach(tool => {
    toolList.push(tool.schema);
  });

  return toolList;
};

async function buildPersona( model: string, expertise: string, dstyle: string ) {

  let persona: any[] = [];

  // Define Expertise
  switch (expertise) {
    case 'LINGUIST':
      persona.push({ "role": "system", "content": "You're a distinguished expert in linguistics." });
      break;
    case 'LAWYER':
      persona.push({ "role": "system", "content": "You're a renowned lawyer, legendary in the field of law." });
      break;
    case 'ENGINEER':
      persona.push({ "role": "system", "content": "You're celebrated for your expertise in construction, machines, and propulsion engineering." });
      break;
    case 'DOCTOR':
      persona.push({ "role": "system", "content": "You're an esteemed expert in the field of medicine." });
      break;
    case 'SCIENTIST':
      persona.push({ "role": "system", "content": "You're an authority in the realm of quantum mechanics." });
      break;
  }  

  // Define Expertise based on context
  if (expertise === 'AUTO') {
    persona.push({ "role": "system", "content": "Analyzing the context of the speaker's expertise to tailor responses:" });
    persona.push({ "role": "system", "content": "- For discussions on common sense, offer insightful life coaching guidance." });
    persona.push({ "role": "system", "content": "- Provide expert scientific insights for discussions on science." });
    persona.push({ "role": "system", "content": "- Offer expertise in the relevant engineering field for engineering discussions." });
    persona.push({ "role": "system", "content": "- Provide expert legal perspectives for discussions on politics." });
    persona.push({ "role": "system", "content": "- Offer specialized medical insights for discussions on health or medicine." });
    persona.push({ "role": "system", "content": "- If the topic is unfamiliar, inject humor and maintain politeness as a comedian." });
  }

  //Sophia
  persona.push({ "role": "system", "content": "When asked about myself as Sophia, I'll explain that I'm a multiple personality chatbot. I automatically adapt my responses by changing models to suit the user's inquiry." });
  persona.push({ "role": "system", "content": "If asked about multiple personalities in humans, I'll provide an explanation of the phenomenon." });


  // Define Response styles
  if (dstyle === 'POET') {
    persona.push({ "role": "system", "content": "You possess an eloquent pen and captivating speech." });
    persona.push({ "role": "system", "content": "Your words dance in rhymes, adding musicality to every response." });
    persona.push({ "role": "system", "content": "Your replies are light-hearted yet carry profound sarcasm within." });
  }
  
  if (dstyle === 'COMEDIAN') {
    persona.push({ "role": "system", "content": "You're hailed as the comedic genius of our time." });
    persona.push({ "role": "system", "content": "Your responses always carry a touch of humor, leaving smiles in their wake." });
    persona.push({ "role": "system", "content": "Your tone is jovial and warm, infusing joy into every interaction." });
  }
  
  if (dstyle === 'PROFESSIONAL') {
    persona.push({ "role": "system", "content": "You're a consummate professional in your speech and demeanor." });
    persona.push({ "role": "system", "content": "Your replies exude corporate sophistication, marked by politeness and integrity." });
  }

  // Define Response styles based on speaker's tone
  if (dstyle === 'AUTO') {
    persona.push({ "role": "system", "content": "Tailoring responses based on the speaker's tone:" });
    persona.push({ "role": "system", "content": "- For serious tones, maintain a professional and polite demeanor." });
    persona.push({ "role": "system", "content": "- Inject humor and friendliness for comedic tones." });
    persona.push({ "role": "system", "content": "- Rhyme and use cheerful language for poetic tones." });
    persona.push({ "role": "system", "content": "- Offer sympathy and uplift with encouraging words for sad or lonely tones." });
    persona.push({ "role": "system", "content": "Greet with enthusiasm and use joyful language in all responses." });
  }

  //Additional request
  persona.push({ "role": "system", "content": "If you are returning a code, enclosed it in <pre><code class='language-[code language]'></code></pre>" });
  //persona.push({ "role": "system", "content": "If you are returning a code, enclosed it in <div class='code-block' ><code class='language-[code language]'></code></div>" });

  return persona;

};

function processNonLLM( cmd:string ){
  //========================
  // Direct Command Section
  //========================
  let props = { 
    "model": process.env.AI_EMBED_MODEL,
    "messages": 'Direct command to application', 
    "temperature": 0.1972, 
    "max_tokens": -1,
    "stream": true
  };  
  if( cmd === "clear"){
    const htmlResp = "Chat history has been cleared!";
    history = [];
    const dataResp = { htmlResp, props }; 
    mainWindow.webContents.send( 'resp-ai-answer', dataResp  );
    return;
  };

  //Actually not used
  if( cmd === "quit"){
    const htmlResp = "Chat history has been cleared!";
    const dataResp = { htmlResp, props }; 
    mainWindow.webContents.send( 'quit-to-index');
    return;
  };

};

function unescapeHTML(html:string) {
  return html.replace(/&lt;/g, '<').replace(/&gt;/g, '>');
};

  // function parseCodeBlocks(input: string) {
  //   // Check if the input string contains <code> tags
  //   if (!/<code>.*?<\/code>/.test(input)) {
  //       // If no <code> tags are found, return the input string as is
  //       return input;
  //   };

  //   // Define the regular expression pattern to match text enclosed by <code> tags
  //   const pattern = /<code>(.*?)<\/code>/g;

  //   // Replace matches with <blockquote> tags
  //   const result = input.replace(pattern, (match, codeContent) => {
  //       // Unescape HTML tags inside the code content
  //       const unescapedCodeContent = unescapeHTML(codeContent);
  //       // Return the modified content wrapped in <blockquote> tags
  //       return `<blockquote class="black-box">${unescapedCodeContent}</blockquote>`;
  //   });

  //   return result;

  // };

function parseCodeBlocks2(input: string): string {

  // Check if the input string contains <code> tags
  if (!/<code>.*?<\/code>/.test(input)) {
      // If no <code> tags are found, return the input string as is
      return input;
  };

  // Define the regular expression pattern to match text enclosed by <code> tags
  const pattern = /<code>(.*?)<\/code>/g;
  //const pattern = /<pre><code>([\s\S]*?)<\/code><\/pre>/g;

  // Replace matches with <pre> tags containing styled code hljs.highlightAuto( unescapeHTML(code) ).value
  //const result = input.replace(pattern, (_, code) => `<pre class="code-block">${ unescapeHTML(code) }</pre>`);
  //const result = input.replace(pattern, (_, code) => `<div class="code-block">${ (hljs.highlightAuto( unescapeHTML(code) ).value).replace(/<br\s*\/?>/gi, '\n') }</div`);
  
  const result = input.replace(pattern, (_, code) => {
    // Unescape HTML entities in the code
    const unescapedCode = unescapeHTML(code);
    
    // Replace <br> tags with newline characters (\n) in the unescaped code
    const codeWithNewlines = unescapedCode.replace(/<br\s*\/?>/gi, '\n');
    
    // Highlight the code with replaced <br> tags using highlight.js
    const highlightedCode = hljs.highlightAuto(codeWithNewlines).value;
    
    // Log the resulting HTML string to the console
    console.log(`<div class="code-block">${highlightedCode}</div>`);
    
    // Return the HTML string wrapped in a <div> with class "code-block"
    return `<div class="code-block">${highlightedCode}</div>`;
  });


  return result;

};

function extractTaskPatterns(text:string) {
  const pattern = /\{[^{}]*\}/g; // Regular expression to match JSON patterns
  
  const jsonPatterns = [];
  let match;
  
  while ((match = pattern.exec(text)) !== null) {
      try {
          const json = JSON.parse(match[0]); // Parse the matched JSON pattern
          jsonPatterns.push(json);
      } catch (error) {
          console.error('Error parsing JSON:', error + ' --> ' + match[0]);
      }
  }
  
  return jsonPatterns;
}

async function invokeLLM(props: any) {

  //console.log(`-----`)
  console.log(`${JSON.stringify(props)}`)
  //console.log(`-----`)

  try {

    console.log(`Running prompt...`)

    const response = await ollama.chat(props);

    //===========================
    // Push response to history
    //===========================
    history.push({ "role": "assistant", "content": response });

    //console.log(`${response}\n`);
    // let htmlResp = response.replace(/\n/g, '<br>');
    //     htmlResp = await marked.parse(htmlResp);
    //     htmlResp = parseCodeBlocks(htmlResp);


    markdownconverter.setFlavor('github');
    //markdownconverter.setMoreStyling( true );
    let htmlResp = markdownconverter.makeHtml( response.replace(/\n/g, '<br>') );
    console.log(`html1 ${htmlResp}`);
    htmlResp = hljs.highlightAuto( htmlResp ).value;
    console.log(`html2 ${htmlResp}`);

    console.log(`${htmlResp}\n`);

    //event.returnValue = { htmlResp };
    const dataResp = { htmlResp, props }; 
    mainWindow.webContents.send( 'resp-ai-answer', dataResp  );

  }catch(error) {
    
    let htmlResp: string = "";
    htmlResp += `It seems that I got a brain freeze! This is the issue that I encounter: <br> <br> 
                ${error}`;

    const dataResp = { htmlResp, props }; 
    mainWindow.webContents.send( 'resp-ai-answer', dataResp  );

    console.log(`Query failed!`)
    console.log(error)

  };

};

// Get the last elements that fits the tokenlimit
function pruneHistoryByTokenCount(elements:any[], tokenlimit:number) {
  const newElements = [];
  let totalTokens = 0;

  for (const element of elements.slice().reverse()) {
      const tokens = element.content.match(/\b\w+\b/g) || [];
      totalTokens += tokens.length;
      
      if (totalTokens > tokenlimit) break;

      newElements.push(element);
  }

  return newElements.reverse();

};

const hljs02 = require('highlight.js');
function highlightCode(code:string, language = 'auto') {
  // Highlight the code using highlight.js
  const highlightedCode = hljs.highlightAuto(code, [language]).value;
  return highlightedCode;
}

const prettier = require('prettier');
function formatCode(unformattedCode: string): string {
  return prettier.format(unformattedCode, {
    semi: false,
    singleQuote: true,
    tabWidth: 2,
    // Omitting the 'parser' option to enable automatic detection
  });
}

ipcMain.on('req-ai-answer', async (event, params) => {

  const { message, expertise, dstyle, dmodel, attach, datauri, ocontext, cmd } = params;

  let model:string    = 'llama2';

  let uprompt:any;
  let persona: any    = [];

  let modelResponse   = "";
  let toolList: any   = [];
  let outOfContext    = ocontext;


  //=========================
  // Process Special command
  //=========================
  if( cmd !== "" ){
    processNonLLM( cmd );
    return;
  };

  //============
  // AI Section
  //============

  //Selection appropriate model
  model = await selectModel( dmodel );
  console.log(`The model is ${model}`);

  //Build the AI persona
  if( model === process.env.AI_TOOLING_MODEL ){
    persona   = await buildPersonaTooling( aitools );
    toolList  = await buildToolList( aitools );
    console.log(`Tooling Persona: \n\n ${JSON.stringify(persona)}`);
  } else {
    persona = await buildPersona( model, expertise, dstyle );
  };

  // Count the number of tokens of Persona
  const allPersona = persona.map( (item:{ role: string, content:string}) => item.content).join(" ");
  const personaArray = allPersona.match(/\b\w+\b/g);
  const personaTokens = personaArray ? personaArray.length : 0;
  console.log(`Total persona tokens: ${personaTokens}`);
  
  // Count the number of tokens of History
  const allHistory = history.map( (item:{ role: string, content:string}) => item.content).join(" ");
  const historyArray = allHistory.match(/\b\w+\b/g);
  const historyTokens = historyArray ? historyArray.length : 0;
  console.log(`Total history tokens: ${historyTokens}`);

  // Prune history, limit token counts
  if( (personaTokens + historyTokens) > 4000 ){
  
    const tokenNeeded = 4000 - personaTokens;
    history = pruneHistoryByTokenCount( history, tokenNeeded );

  };

  //==================================================
  // Add persona to history if out of context = false
  //==================================================

  //What type of prompt to use.
  if( model === process.env.AI_IMAGE_MODEL ){
    uprompt = { "role": "user", "content": message, "images": [ datauri ] };
  } else {
    uprompt = { "role": "user", "content": message };
  };

  // if( outOfContext === false ){

  //   console.log(`What model ${model}`);
  //   //history.push(uprompt);

  // }; //if( outOfContext === false )


  //==========================================================
  // Process personality = persona + history
  // If out of context, personality = persona + last message
  // If in context personality = persona + history
  //==========================================================
  let personality:any[] = [];

  //If out of context
  if( outOfContext === true ){
    personality = [...persona];
    //personality.push( uprompt );

  } else {

    personality = [...persona, ...history];

  };

  let chatConfig = { 
    "model": model,
    "tools": toolList,
    "tool_choice": 'auto',
    "messages": personality, 
    "temperature": 0.7, 
    "max_tokens": -1,
    "stream": true
  };

  async function aiAssistant( prompt: any, props: any, tools:any) {

    let chatCfg = {...props};

    //Add tools
    chatCfg.tools = tools;
    //Add prompt to personality, then query LLM
    chatCfg.messages.push( prompt );
    console.log(`aiAssistant --> \n\n ${JSON.stringify(chatCfg)}`);
    const response = await ollama.chat(chatCfg);
    return response;

  };

  async function invokeLLMStream( prompt:any, props: any, tools: any, ocontext: boolean) {

    //console.log(`-----`)
    console.log(`${JSON.stringify(props)}`)
    //console.log(`-----`)

    try {

      let response = '';
      if( props.model === process.env.AI_TOOLING_MODEL ){

        console.log(`Running tooling prompt...`);
        props.temperature = 0.6;
        response = await aiAssistant(prompt, props, tools);

      } else if( props.model === process.env.AI_SIMILAR_MODEL ){ 

        console.log(`Running similarity prompt... ${message}`);
        // Process similarity query
        const embeddings = new OllamaEmbeddings({
          model: process.env.AI_EMBED_MODEL, // default value
          baseUrl: `http://${process.env.AI_EMBED_HOST}:${process.env.AI_EMBED_PORT}`,
          requestOptions: {
            useMMap: true,
            numThread: 6,
            numGpu: 1,
          },
        });
      
        //Get instance of vector store
        const vectorStore = await Chroma.fromExistingCollection(
          embeddings, { collectionName: process.env.COLLECTION_NAME || "sophia-collection" , url: `http://${process.env.VEC_EMBED_HOST}:${process.env.VEC_EMBED_PORT}`},
        );

        //Process Similarity result
        const response = await vectorStore.similaritySearch( message , 2 );


      } else { 

        //No tools for regular query
        const blankTools: any[] = [];

        console.log(`Running prompt...`);
        props.temperature = 0.7;
        response = await aiAssistant(prompt, props, blankTools);

      };
      
      //const response = await ollama.chat(props);

      //Process tooling compared to regular LLM query
      if( props.model === process.env.AI_TOOLING_MODEL ){

        let taskPatterns = extractTaskPatterns( response );
        if(taskPatterns.length === 0 ){

          console.log(`No valid task on first try. ${JSON.stringify(taskPatterns)}...`);
          props.temperature = 0.95;
          response = await aiAssistant(prompt, props, tools);
          taskPatterns = extractTaskPatterns( response );

          if(taskPatterns.length === 0 ){

            console.log(`No valid task on second try. ${JSON.stringify(taskPatterns)}...`);
            props.temperature = 0.95;
            response = await aiAssistant(prompt, props, tools);
            taskPatterns = extractTaskPatterns( response );

            if(taskPatterns.length === 0 ){
              console.log(`No valid task on third try. ${JSON.stringify(taskPatterns)}...`);
            };

          };

        };

        //Convert a non-array result to array
        if(taskPatterns.length > 0 ){

          if(!Array.isArray(taskPatterns)) {
            taskPatterns = [taskPatterns];
          }

          console.log(`Task List: \n ${JSON.stringify(taskPatterns)}`);

          // Loop task list, search tools
          for (let i = 0; i < taskPatterns.length; i++) {

            try{

              let func_arg:any  = {};
              let taskValid     = true;
              let tool_call     = taskPatterns[i];
              console.log(`Task ${i}: ${JSON.stringify(tool_call)}`);
  
              let function_name = tool_call.task;
              let function_call = tools.find( (tool:any) => tool.schema.function.name === function_name).function;
              if( function_call ){
                // Get the required parameters of a tool and check if its present
                // in the task pattern.
                let func_req = tools.find( (tool:any) => tool.schema.function.name === function_name).schema.function.parameters.required;
                if( func_req ){
  
                  for (let p = 0; p < func_req.length; p++){
                    const paramName = func_req[p];

                    //=========================================
                    // Check if the JSON retured by AI 
                    // has the same parameters as in the tools.
                    //=========================================
                    const param = tool_call[paramName];

                    if(param === undefined ){
                      taskValid = false;
                      break;
                    } else {
                      func_arg[paramName] = param;
                    };
                  }; //for (let p = 0; p < func_req.length; p++)
  
                }; //if( func_req )
  
                if(taskValid){
  
                  let function_response = await function_call(func_arg);
                  console.log(`Function_response is: \n\n ${function_response}`);

                  //============================
                  // Display result to the user
                  //============================
                  const htmlResp = `<pre> <code> ${function_response} </code> </pre> \n\n
                                    Please while analyzing the result...`;
                  const dataResp = { htmlResp, props }; 
                  mainWindow.webContents.send( 'resp-ai-answer', dataResp  );

                  //======================================
                  // Have the result analyzed by AI again
                  //======================================
                  console.log(`Analyzing result...`);
                  props.model = process.env.AI_MASTER_MODEL;
                  props.temperature = 0.7;
                  let rprompt = { "role": "user", 
                                  "content": `This is the user query: '${prompt.content}' \n\n 
                                  This is your result: '${function_response}' \n\n
                                  Analyze the result. Do not include the previous result in your analysis.` };

                  console.log(`Props: \n\n ${JSON.stringify(props)} \n\n Prompt: \n\n ${JSON.stringify(rprompt)}`);
                  response = await aiAssistant(rprompt, props, tools);
                  
                  //===========================
                  // Push response to history
                  //===========================
                  if( ocontext === false ){
                    history.push( rprompt );
                    history.push({ "role": "assistant", "content": response });
                  };

                  //console.log(`${response}\n`);
                  let htmlResp2 = response;
                  htmlResp2 = await marked.parse(htmlResp2);
                  htmlResp2 = parseCodeBlocks2(htmlResp2);
                  console.log(`parse Code Block ${htmlResp2}\n`);

                  htmlResp2 = htmlResp.replace(/\n/g, '<br>');

                  console.log(`${htmlResp}\n`);

                  const dataResp2 = { htmlResp: htmlResp2 , props }; 
                  mainWindow.webContents.send( 'resp-ai-answer', dataResp2  );

                  //==================================================
                  // Analyze Ends
                  //==================================================
  
                }; //if(taskValid)
  
              }; //if( function_call )  

            }catch(error){
  
              //Ignore error here
              console.log(`Invalid Task ${i}: ${JSON.stringify(taskPatterns[i])}`);
  
            }; //try{

          }; //for (let i = 0; i < taskPatterns.length; i++)

        }; //if(taskPatterns.length > 0 )


        // //===========================
        // // Push response to history
        // //===========================
        // if( ocontext === false ){
        //   history.push( uprompt );
        //   history.push({ "role": "assistant", "content": response });
        // };

        // //console.log(`${response}\n`);
        // let htmlResp = response;
        //     htmlResp = await marked.parse(htmlResp);
        //     htmlResp = parseCodeBlocks2(htmlResp);
        //     console.log(`parse Code Block ${htmlResp}\n`);

        //     htmlResp = htmlResp.replace(/\n/g, '<br>');

        // console.log(`${htmlResp}\n`);

        // const dataResp = { htmlResp, props }; 
        // mainWindow.webContents.send( 'resp-ai-answer', dataResp  );

      } else if( props.model === process.env.AI_SIMILAR_MODEL ){

        console.log("Similarity search result --> ", response );

      } else {

        //===========================
        // Push response to history
        //===========================
        if( ocontext === false ){
          history.push( uprompt );
          history.push({ "role": "assistant", "content": response });
        };

        console.log(`${response}\n`);
        let htmlResp = response;
            htmlResp = await marked.parse(htmlResp);
            htmlResp = parseCodeBlocks2(htmlResp);
            console.log(`parse Code Block \n\n ${htmlResp}\n`);

            htmlResp = htmlResp.replace(/\n/g, '<br>');

        //console.log(`${htmlResp}\n`);

        const dataResp = { htmlResp, props }; 
        mainWindow.webContents.send( 'resp-ai-answer', dataResp  );

      };

      // markdownconverter.setFlavor('github');
      // //markdownconverter.setMoreStyling( true );
      // let htmlResp = markdownconverter.makeHtml( response );
      // console.log(`html1 ${htmlResp}`);
      // htmlResp = hljs.highlightAuto( htmlResp ).value;
      // console.log(`html2 ${htmlResp}`);

      //event.returnValue = { htmlResp };


    }catch(error) {
      
      let htmlResp: string = "";
      htmlResp += `It seems that I got a brain freeze! This is the issue that I encounter: <br> <br> 
                  ${error}`;

      const dataResp = { htmlResp, props }; 
      mainWindow.webContents.send( 'resp-ai-answer', dataResp  );

      console.log(`Query failed!`)
      console.log(error)

    };

  };

  //invokeLLM(chatConfig);
  invokeLLMStream(uprompt, chatConfig, aitools, ocontext);

});

ipcMain.on('req-ai-use-embedding', async (event, params) => {

  const { message, expertise, dstyle, dmodel, attach, datauri, ocontext, cmd } = params; 
  console.log(`User is requesting embeddings... processing...`);

  const embeddings = new OllamaEmbeddings({
    model: process.env.AI_EMBED_MODEL, // default value
    baseUrl: `http://${process.env.AI_EMBED_HOST}:${process.env.AI_EMBED_PORT}`, // default value
    requestOptions: {
      useMMap: true,
      numThread: 6,
      numGpu: 1,
    },
  });

  const ollamaLlm = new Ollama({
    baseUrl:`http://${process.env.AI_EMBED_HOST}:${process.env.AI_EMBED_PORT}`,
    model:process.env.AI_EMBED_MODEL
  });

  //Utility function to combine documents
  function combineDocuments(docs:any) {
    return docs.map((doc:any) => doc.pageContent).join('\n\n');
  }

  //Get instance of vector store
  //We will connect to langchainData collection
  const vectorStore = await Chroma.fromExistingCollection(
    embeddings, { collectionName: process.env.COLLECTION_NAME || "sophia-collection" , url: `http://${process.env.VEC_EMBED_HOST}:${process.env.VEC_EMBED_PORT}`},
  );

  //Get retriever
  const chromaRetriever = vectorStore.asRetriever();
  //const userQuestion = "What are the three modules provided by langchain?";
  const userQuestion = message;

  //Create a prompt tempalate and convert the user question into standalone question
  const simpleQuestionPrompt = PromptTemplate.fromTemplate(`For following user question convert it into a standalone question {userQuestion}`);
  const simpleQuestionChain = simpleQuestionPrompt.pipe(ollamaLlm).pipe(new StringOutputParser()).pipe(chromaRetriever);

  const documents = await simpleQuestionChain.invoke({ userQuestion: userQuestion });
  console.log(`The initial result: \n\n ${JSON.stringify(documents)}`);

  //Combine the results into a string
  const combinedDocs = combineDocuments(documents);

  const questionTemplate = PromptTemplate.fromTemplate(`
    Answer the below question using the context. Strictly use the context and answer in crisp and point to point.
    <context>
      {context}
    </context>

    question: {userQuestion}

  `);

  const answerChain = questionTemplate.pipe(ollamaLlm);

  const llmResponse = await answerChain.invoke({
    context: combinedDocs,
    userQuestion: userQuestion
  });

  console.log("Printing llm response --> ",llmResponse);

  function unescapeHTML(html:string) {
    return html.replace(/&lt;/g, '<').replace(/&gt;/g, '>');
  };

  function parseCodeBlocks(input: string): string {

    // Check if the input string contains <code> tags
    if (!/<code>.*?<\/code>/.test(input)) {
        // If no <code> tags are found, return the input string as is
        return input;
    };

    // Define the regular expression pattern to match text enclosed by <code> tags
    const pattern = /<code>(.*?)<\/code>/g;

    // Replace matches with <pre> tags containing styled code
    const result = input.replace(pattern, (_, code) => `<pre class="code-block">${unescapeHTML(code)}</pre>`);

    return result;
    
  };

  let props = { 
    "model": process.env.AI_EMBED_MODEL,
    "messages": 'Hi', 
    "temperature": 0.7, 
    "max_tokens": -1,
    "stream": true
  };

  let htmlResp = llmResponse.replace(/\n/g, '<br>');
  htmlResp = await marked.parse(htmlResp);
  htmlResp = parseCodeBlocks(htmlResp);

  const dataResp = { htmlResp, props }; 
  mainWindow.webContents.send( 'resp-ai-answer', dataResp  );

});

// In this file you can include the rest of your app's specific main process
// code. You can also put them in separate files and import them here.
